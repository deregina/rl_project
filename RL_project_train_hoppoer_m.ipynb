{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym==0.21.0\n",
      "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m error in gym setup command: 'extras_require' must be a dictionary whose values are strings or lists of strings containing valid project/version requirement specifiers.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDqW4nZPbQZS",
    "outputId": "b4e4f388-2c4e-4cef-aaad-6837602f7770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym==0.21.0\n",
      "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Collecting free-mujoco-py\n",
      "  Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Cython<0.30.0,>=0.29.24 (from free-mujoco-py)\n",
      "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.16.0)\n",
      "Collecting fasteners==0.15 (from free-mujoco-py)\n",
      "  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n",
      "Collecting glfw<2.0.0,>=1.4.0 (from free-mujoco-py)\n",
      "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.31.6)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.25.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.16.0)\n",
      "Collecting monotonic>=0.1 (from fasteners==0.15->free-mujoco-py)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.22)\n",
      "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (9.4.0)\n",
      "Installing collected packages: monotonic, glfw, fasteners, Cython, free-mujoco-py\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 3.0.10\n",
      "    Uninstalling Cython-3.0.10:\n",
      "      Successfully uninstalled Cython-3.0.10\n",
      "Successfully installed Cython-0.29.37 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n",
      "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.4.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (67.7.2)\n",
      "Collecting colabgymrender==1.0.2\n",
      "  Downloading colabgymrender-1.0.2.tar.gz (1.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pyvirtualdisplay (from colabgymrender==1.0.2)\n",
      "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from colabgymrender==1.0.2) (1.0.3)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from colabgymrender==1.0.2) (0.25.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from colabgymrender==1.0.2) (4.8.0.76)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->colabgymrender==1.0.2) (1.25.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->colabgymrender==1.0.2) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->colabgymrender==1.0.2) (0.0.8)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (2.31.6)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->colabgymrender==1.0.2) (0.4.9)\n",
      "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender==1.0.2) (9.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy->colabgymrender==1.0.2) (67.7.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (2024.2.2)\n",
      "Building wheels for collected packages: colabgymrender\n",
      "  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for colabgymrender: filename=colabgymrender-1.0.2-py3-none-any.whl size=2425 sha256=083b40b6ca28518386bf5ce1e5582cc4f7bb4321f07f5fb125dd53bfcf1a9749\n",
      "  Stored in directory: /root/.cache/pip/wheels/54/1d/51/d65a99106096dd07e3be6f08d5d3c2dd5f26ac98f626e5efc9\n",
      "Successfully built colabgymrender\n",
      "Installing collected packages: pyvirtualdisplay, colabgymrender\n",
      "Successfully installed colabgymrender-1.0.2 pyvirtualdisplay-3.0\n",
      "Collecting xvfbwrapper\n",
      "  Downloading xvfbwrapper-0.2.9.tar.gz (5.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: xvfbwrapper\n",
      "  Building wheel for xvfbwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for xvfbwrapper: filename=xvfbwrapper-0.2.9-py3-none-any.whl size=5009 sha256=95deef0667a639ebaa7dc8d5420eafcf42a2e5e19c52e8187a4aba96dd9cbf05\n",
      "  Stored in directory: /root/.cache/pip/wheels/cb/10/7d/2b7fdffccf837f7d5425931575fbee9caebe2c190931f9058b\n",
      "Successfully built xvfbwrapper\n",
      "Installing collected packages: xvfbwrapper\n",
      "Successfully installed xvfbwrapper-0.2.9\n",
      "Collecting imageio==2.4.1\n",
      "  Downloading imageio-2.4.1.tar.gz (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.1) (1.25.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imageio==2.4.1) (9.4.0)\n",
      "Building wheels for collected packages: imageio\n",
      "  Building wheel for imageio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for imageio: filename=imageio-2.4.1-py3-none-any.whl size=3303885 sha256=b859de90d44abcef4478930c8a6884d0bcd9d528e648371206477ed4943f6a6a\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/5d/ce/bdbdb04744dac03906336eb0d01ff1e222061d3419c55c55f9\n",
      "Successfully built imageio\n",
      "Installing collected packages: imageio\n",
      "  Attempting uninstall: imageio\n",
      "    Found existing installation: imageio 2.31.6\n",
      "    Uninstalling imageio-2.31.6:\n",
      "      Successfully uninstalled imageio-2.31.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "free-mujoco-py 2.1.6 requires imageio<3.0.0,>=2.9.0, but you have imageio 2.4.1 which is incompatible.\n",
      "moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= \"3.4\", but you have imageio 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed imageio-2.4.1\n",
      "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (0.4.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg) (67.7.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !pip install gymnasium==0.26.1\n",
    "!pip install free-mujoco-py\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install imageio-ffmpeg\n",
    "\n",
    "!pip install colabgymrender==1.0.2\n",
    "!pip install xvfbwrapper\n",
    "!pip install imageio==2.4.1\n",
    "!pip install imageio-ffmpeg\n",
    "!pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "t1CUUi8fewvG"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/sejeong/Documents/Spring2024/ECEN743_RL/Project/generalized_dt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eCI72lQPnohW",
    "outputId": "5ffc2143-2583-4da6-e683-2d28c4f822a6"
   },
   "outputs": [],
   "source": [
    "# !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "14-I3V0gopnY"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['PATH']=\"$HOME/.cargo/bin:\"+'/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cFzAKJ7coE7V"
   },
   "outputs": [],
   "source": [
    "# !export PATH=\"$HOME/.cargo/bin:$PATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lhudoHx1n715"
   },
   "outputs": [],
   "source": [
    "# !export PATH=\"$HOME/.cargo/bin:$PATH\" && pip3 install -v transformers==4.15.0 timm==0.4.12 fairscale==0.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZN9dbJpInplz"
   },
   "outputs": [],
   "source": [
    "# !source \"$HOME/.cargo/env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cLy8JyuMnoe1"
   },
   "outputs": [],
   "source": [
    "# curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "# after finish source \"$HOME/.cargo/env\"\n",
    "# and pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GHrrYsz0m5Zi"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Gdza1EyUy9xV"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from decision_transformer.models.decision_transformer import GeneralizedDecisionTransformer\n",
    "from decision_transformer.training.seq_trainer import CategoricalSequenceTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVQYuBkDzCdu",
    "outputId": "9a7c7501-b1fa-439b-ffe2-cd2100b41a2b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "VELOCITY_DIM = {\n",
    "    'halfcheetah': (8, ),\n",
    "    'hopper': (5, ),\n",
    "    'walker2d': (8, ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ci2DCV152M5k"
   },
   "outputs": [],
   "source": [
    "def discount_cumsum(x, gamma):\n",
    "    discount_cumsum = np.zeros_like(x)\n",
    "    discount_cumsum[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0]-1)):\n",
    "        discount_cumsum[t] = x[t] + gamma * discount_cumsum[t+1]\n",
    "    return discount_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "m8MEYqIi2PoX"
   },
   "outputs": [],
   "source": [
    "def experiment(output_dir, variant):\n",
    "    gpu = variant.get('gpu', 0)\n",
    "    device = torch.device(\n",
    "        f\"cuda:{gpu}\" if (torch.cuda.is_available() and gpu >= 0) else \"cpu\"\n",
    "    )\n",
    "\n",
    "    env_name, dataset = variant['env'], variant['dataset']\n",
    "    seed = variant['seed']\n",
    "    dist_dim = variant['dist_dim']\n",
    "    n_bins = variant['n_bins']\n",
    "    distributions = variant['distributions']\n",
    "    assert distributions in ['categorical', 'deterministic']\n",
    "    gamma = variant['gamma']\n",
    "    if distributions != 'categorical':\n",
    "        assert gamma == 1.\n",
    "    condition = variant['condition']\n",
    "    assert condition in ['reward', 'xvel']\n",
    "\n",
    "    if env_name == 'hopper':\n",
    "        env = gym.make('Hopper-v3')\n",
    "    elif env_name == 'halfcheetah':\n",
    "        env = gym.make('HalfCheetah-v3')\n",
    "    elif env_name == 'walker2d':\n",
    "        env = gym.make('Walker2d-v3')\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    vel_dim = VELOCITY_DIM[env_name]\n",
    "    scale = 1000.\n",
    "    max_ep_len = 1000\n",
    "    env.seed(seed)\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    if distributions == 'categorical':\n",
    "        r_dists_dim = dist_dim\n",
    "    elif distributions == 'deterministic':\n",
    "        r_dists_dim = 1\n",
    "\n",
    "    dataset_path = f'data/{env_name}-{dataset}-v2.pkl'\n",
    "\n",
    "    with open(dataset_path, 'rb') as f:\n",
    "        trajectories = pickle.load(f)\n",
    "\n",
    "    states, traj_lens, returns, rewards = [], [], [], []\n",
    "    for path in trajectories:\n",
    "        states.append(path['observations'])\n",
    "        traj_lens.append(len(path['observations']))\n",
    "        returns.append(path['rewards'].sum())\n",
    "        if condition == 'reward':\n",
    "            rewards.extend(path['rewards'])\n",
    "        elif condition == 'xvel':\n",
    "            rewards.extend(path['observations'][:, vel_dim[0]])\n",
    "    traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "    # for categorical distribution matching\n",
    "    r_min = min(rewards)\n",
    "    r_max = max(rewards)\n",
    "    bins = np.linspace(r_min, r_max, n_bins)\n",
    "    label = [(bins[i]+bins[i+1])/2 for i in range(len(bins)-1)]\n",
    "\n",
    "    # used for input normalization\n",
    "    states = np.concatenate(states, axis=0)\n",
    "    state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "    num_timesteps = sum(traj_lens)\n",
    "\n",
    "    print('=' * 50)\n",
    "    print(f'Starting new experiment: {env_name} {dataset}')\n",
    "    print(f'{len(traj_lens)} trajectories, {num_timesteps} timesteps found')\n",
    "    print(f'Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}')\n",
    "    print(f'Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}')\n",
    "    print(f'Modality: {condition}')\n",
    "    print(f'Distribution: {distributions}')\n",
    "    print('=' * 50)\n",
    "\n",
    "    K = variant['K']\n",
    "    batch_size = variant['batch_size']\n",
    "\n",
    "    print('Preparing empirical distributions.')\n",
    "    # for evaluation with best/50% trajectories\n",
    "    _idxes = np.argsort([np.sum(path['rewards']) for path in trajectories]) # rank 0 is the most bad demo.\n",
    "    trajs_rank = np.empty_like(_idxes)\n",
    "    trajs_rank[_idxes] = np.arange(len(_idxes))\n",
    "    n_evals = 5\n",
    "\n",
    "    r_dists = []\n",
    "    if condition in ('reward', 'xvel') and distributions == 'categorical':\n",
    "        for path in trajectories:\n",
    "            dist = np.zeros(n_bins - 1)\n",
    "            distributional_rewards = []\n",
    "            steps_to_go = 0\n",
    "            if condition == 'reward':\n",
    "                modality = path['rewards']\n",
    "            elif condition == 'xvel':\n",
    "                modality = path['observations'][:, vel_dim[0]]\n",
    "            for t, r in enumerate(reversed(modality)):\n",
    "                discretized_r = np.histogram(np.clip(r, r_min, r_max), bins=bins)[0]\n",
    "                steps_to_go *= gamma\n",
    "                dist *= steps_to_go\n",
    "                dist = discretized_r + dist\n",
    "                dist_norm = dist.sum()\n",
    "                dist /= dist_norm\n",
    "                steps_to_go += 1\n",
    "                distributional_rewards.append(dist)\n",
    "            path['r_dists'] = np.concatenate(distributional_rewards[::-1], axis=0).reshape(-1, n_bins - 1)\n",
    "            r_dists.append(path['r_dists'])\n",
    "    elif condition in ('reward', 'xvel') and distributions == 'deterministic':\n",
    "        for path in trajectories:\n",
    "            dist = 0\n",
    "            distributional_rewards = []\n",
    "            if condition == 'reward':\n",
    "                modality = path['rewards']\n",
    "            elif condition == 'xvel':\n",
    "                modality = path['observations'][:, vel_dim[0]]\n",
    "            for t, r in enumerate(reversed(modality)):\n",
    "                if t == 0:\n",
    "                    dist += r\n",
    "                else:\n",
    "                    dist = r + gamma * dist\n",
    "                distributional_rewards.append(dist)\n",
    "            path['r_dists'] = np.array(distributional_rewards[::-1]).reshape(-1, 1) / max_ep_len\n",
    "            r_dists.append(path['r_dists'])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    assert len(trajs_rank) == len(r_dists)\n",
    "    # train / eval split\n",
    "    eval_indices = [np.where(trajs_rank == len(trajs_rank)-idx-1)[0][0] for idx in range(n_evals)] + [np.where(trajs_rank == int(len(trajs_rank)/2)+idx-2)[0][0] for idx in range(n_evals)]\n",
    "    # remove eval trajectories\n",
    "    train_indices =  [i for i in range(len(trajs_rank))]\n",
    "    for i in eval_indices:\n",
    "        train_indices.remove(i)\n",
    "\n",
    "    def get_batch(batch_size=256, max_len=K):\n",
    "        batch_inds = np.random.choice(\n",
    "            np.array(train_indices),\n",
    "            size=batch_size,\n",
    "            replace=True,\n",
    "        )\n",
    "        s, a, r, d, rtg, timesteps, mask, dist = [], [], [], [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            traj = trajectories[int(batch_inds[i])]\n",
    "            si = random.randint(0, traj['rewards'].shape[0] - 1)\n",
    "\n",
    "            s.append(traj['observations'][si:si + max_len].reshape(1, -1, state_dim))\n",
    "            a.append(traj['actions'][si:si + max_len].reshape(1, -1, act_dim))\n",
    "            r.append(traj['rewards'][si:si + max_len].reshape(1, -1, 1))\n",
    "            if 'terminals' in traj:\n",
    "                d.append(traj['terminals'][si:si + max_len].reshape(1, -1))\n",
    "            else:\n",
    "                d.append(traj['dones'][si:si + max_len].reshape(1, -1))\n",
    "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "            timesteps[-1][timesteps[-1] >= max_ep_len] = max_ep_len-1  # padding cutoff\n",
    "            rtg.append(discount_cumsum(traj['rewards'][si:], gamma=1.)[:s[-1].shape[1] + 1].reshape(1, -1, 1))\n",
    "            if rtg[-1].shape[1] <= s[-1].shape[1]:\n",
    "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "            if condition in ('reward', 'xvel') and distributions == 'categorical':\n",
    "                dist.append(traj['r_dists'][si:si + max_len].reshape(1, -1, dist_dim))\n",
    "                batch_dist_dim = dist_dim\n",
    "            elif condition in ('reward', 'xvel') and distributions == 'deterministic':\n",
    "                dist.append(traj['r_dists'][si:si + max_len].reshape(1, -1, 1))\n",
    "                batch_dist_dim = 1\n",
    "            tlen = s[-1].shape[1]\n",
    "            s[-1] = np.concatenate([np.zeros((1, max_len - tlen, state_dim)), s[-1]], axis=1)\n",
    "            s[-1] = (s[-1] - state_mean) / state_std\n",
    "            a[-1] = np.concatenate([np.ones((1, max_len - tlen, act_dim)) * -10., a[-1]], axis=1)\n",
    "            r[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), r[-1]], axis=1)\n",
    "            d[-1] = np.concatenate([np.ones((1, max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "            rtg[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), rtg[-1]], axis=1)\n",
    "            timesteps[-1] = np.concatenate([np.zeros((1, max_len - tlen)), timesteps[-1]], axis=1)\n",
    "            mask.append(np.concatenate([np.zeros((1, max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
    "            dist[-1] = np.concatenate([np.zeros((1, max_len - tlen, batch_dist_dim)), dist[-1]], axis=1)\n",
    "\n",
    "        s = torch.from_numpy(np.concatenate(s, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        a = torch.from_numpy(np.concatenate(a, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        r = torch.from_numpy(np.concatenate(r, axis=0)).to(dtype=torch.float32, device=device)\n",
    "        d = torch.from_numpy(np.concatenate(d, axis=0)).to(dtype=torch.long, device=device)\n",
    "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).to(dtype=torch.float32, device=device) / scale\n",
    "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).to(dtype=torch.long, device=device)\n",
    "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).to(device=device)\n",
    "        dist = torch.from_numpy(np.concatenate(dist, axis=0)).to(dtype=torch.float32, device=device)\n",
    "\n",
    "        return s, a, r, d, rtg, timesteps, mask, dist\n",
    "\n",
    "    model = GeneralizedDecisionTransformer(\n",
    "        state_dim=state_dim,\n",
    "        act_dim=act_dim,\n",
    "        max_length=K,\n",
    "        max_ep_len=max_ep_len,\n",
    "        hidden_size=variant['embed_dim'],\n",
    "        dist_dim=r_dists_dim,\n",
    "        n_layer=variant['n_layer'],\n",
    "        n_head=variant['n_head'],\n",
    "        n_inner=4*variant['embed_dim'],\n",
    "        activation_function=variant['activation_function'],\n",
    "        n_positions=1024,\n",
    "        resid_pdrop=variant['dropout'],\n",
    "        attn_pdrop=variant['dropout'],\n",
    "    )\n",
    "\n",
    "    model = model.to(device=device)\n",
    "\n",
    "    warmup_steps = variant['warmup_steps']\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=variant['learning_rate'],\n",
    "        weight_decay=variant['weight_decay'],\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lambda steps: min((steps+1)/warmup_steps, 1)\n",
    "    )\n",
    "\n",
    "    trainer = CategoricalSequenceTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        batch_size=batch_size,\n",
    "        get_batch=get_batch,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=lambda s_hat, a_hat, r_hat, s, a, r: torch.mean((a_hat - a)**2),\n",
    "        eval_fns=None,\n",
    "    )\n",
    "\n",
    "    print('Starting training loop.')\n",
    "    for itr in range(variant['max_iters']):\n",
    "        outputs = trainer.train_only_iteration(num_steps=variant['num_steps_per_iter'], iter_num=itr+1, print_logs=True)\n",
    "        if variant['save_model']:\n",
    "            torch.save(model.state_dict(), os.path.join(output_dir, f'dt_{itr}.pth'))\n",
    "        if itr == 0:\n",
    "            _basic_columns = ['iter']\n",
    "            _record_values = [itr]\n",
    "            for k, v in outputs.items():\n",
    "                _basic_columns.append(k)\n",
    "                _record_values.append(v)\n",
    "            with open(os.path.join(output_dir, \"train_log.txt\"), \"w\") as f:\n",
    "                print(\"\\t\".join(_basic_columns), file=f)\n",
    "            with open(os.path.join(output_dir, \"train_log.txt\"), \"a+\") as f:\n",
    "                print(\"\\t\".join(str(x) for x in _record_values), file=f)\n",
    "        else:\n",
    "            _record_values = [itr]\n",
    "            for v in outputs.values():\n",
    "                _record_values.append(v)\n",
    "            with open(os.path.join(output_dir, \"train_log.txt\"), \"a+\") as f:\n",
    "                print(\"\\t\".join(str(x) for x in _record_values), file=f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdvSiI7l2Rw1",
    "outputId": "4b363d26-5fc0-494f-d665-1b218dd1b171"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment Hopper-v3 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/envs/mujoco/mujoco_env.py:237: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "\n",
      "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "\n",
      "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "\n",
      "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  deprecation(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting new experiment: hopper medium\n",
      "2186 trajectories, 999906 timesteps found\n",
      "Average return: 1422.06, std: 378.95\n",
      "Max return: 3222.36, min: 315.87\n",
      "Modality: reward\n",
      "Distribution: categorical\n",
      "==================================================\n",
      "Preparing empirical distributions.\n",
      "Starting training loop.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env', type=str, default='hopper')\n",
    "    parser.add_argument('--dataset', type=str, default='medium')\n",
    "    parser.add_argument('--K', type=int, default=20)\n",
    "    parser.add_argument('--batch_size', type=int, default=64)\n",
    "    parser.add_argument('--embed_dim', type=int, default=128)\n",
    "    parser.add_argument('--n_layer', type=int, default=3)\n",
    "    parser.add_argument('--n_head', type=int, default=1)\n",
    "    parser.add_argument('--activation_function', type=str, default='relu')\n",
    "    parser.add_argument('--dropout', type=float, default=0.1)\n",
    "    parser.add_argument('--learning_rate', '-lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--weight_decay', '-wd', type=float, default=1e-4)\n",
    "    parser.add_argument('--warmup_steps', type=int, default=10000)\n",
    "    parser.add_argument('--max_iters', type=int, default=10)\n",
    "    parser.add_argument('--num_steps_per_iter', type=int, default=10000)\n",
    "    parser.add_argument('--gpu', type=int, default=0)\n",
    "    parser.add_argument('--seed', type=int, default=0)\n",
    "    parser.add_argument('--dist_dim', type=int, default=30)\n",
    "    parser.add_argument('--n_bins', type=int, default=31)\n",
    "    parser.add_argument('--gamma', type=float, default=1.00)\n",
    "    parser.add_argument('--save_model', type=bool, default=False)\n",
    "    parser.add_argument('--condition', type=str, default='reward')  # xvel\n",
    "    parser.add_argument('--distributions', type=str, default='categorical')  # deterministic\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # random seed\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    # log dir\n",
    "    save_dir = f'{args.env}-{args.dataset}-{args.distributions}-dim_{args.dist_dim}-bin_{args.n_bins}-gamma_{args.gamma}-{args.condition}-ctx_{args.K}-seed_{args.seed}'\n",
    "    output_dir = os.path.join('./results', save_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(output_dir, 'params.json'), mode=\"w\") as f:\n",
    "        json.dump(args.__dict__, f, indent=4)\n",
    "\n",
    "    experiment(output_dir, variant=vars(args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DByW1qEC2c0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHnb6oA-C2aA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2bYmY8oC2T6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRqmKbcICgAZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
